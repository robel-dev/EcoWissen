{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ec0556-d85a-4c58-a098-1b5c994a4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from pyprojroot import here\n",
    "import yaml\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d4d31e-60eb-4aea-8b4f-355657776af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/config.yml\") as cfg:\n",
    "    app_config = yaml.load(cfg, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c486197-14db-4ae6-9db4-af5ebb3da6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_data_dir': {'dir': 'data/raw',\n",
       "  'technical_support_pdf_dir': 'data/raw/technical support.pdf'},\n",
       " 'json_dir': {'dir': 'data/json',\n",
       "  'technical_support_qa': 'data/json/technical_support_qa.json',\n",
       "  'product_user_manual_instruction_response': 'data/json/product_user_manual.json'},\n",
       " 'interim_dir': {'dir': 'data/interim',\n",
       "  'cubetriangle_qa': 'data/interim/output.json',\n",
       "  'cubetriangle_instruction_response': 'data/interim/cubetriangle_instruction_response.jsonl'},\n",
       " 'model_dir': {'llama2_7b': 'models/converted_llama_models/llama-2-7b',\n",
       "  'llama2_7b_chat': 'models/converted_llama_models/llama-2-7b-chat'},\n",
       " 'finetuned_model_dir': 'models/fine_tuned_models/CubeTriangle_{}_{}_{}',\n",
       " 'llama_cfg': {'max_seq_len': 512, 'max_batch_size': 6},\n",
       " 'data_type': 'qa_in_input_ids_qa_in_label',\n",
       " 'llm_function_caller': {'gpt_model': 'gpt-35-turbo-16k',\n",
       "  'temperature': 0,\n",
       "  'system_role': 'You are a helpful CubeTriangle chatbot. Your goal is to interact with customers and treat them respectfully. Greet them and ask them how you can help them.Make it clear that your reponsibility is to only provide them with a proper response to their questions about CubeTriangl company and its products.When the customer asked its question about CubeTrianlge, do not use your own knoledge and instead pass the query to the function that you have access to.'},\n",
       " 'cubetriangle_llm_config': {'model_path': 'openlm-research/open_llama_3b',\n",
       "  'finetuned_model_dir': 'models/fine_tuned_models/CubeTriangle_open_llama_3b_2e_qa_qa',\n",
       "  'max_input_tokens': 1000,\n",
       "  'max_length': 100},\n",
       " 'llm_inference': {'gpt_model': 'gpt-35-turbo-16k',\n",
       "  'temperature': 0,\n",
       "  'system_role': \"You will recieve the chat history, user's new query, along with the Cubetriangle's private Large Language Model's response. The response might contain redundant orimproper parts. Extract a proper response from it and provide the user with the most relevant information.\\n\\n\"},\n",
       " 'memory': {'directory': 'memory/chat_history_{}.csv', 'num_entries': 2}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf90108f-2c63-40bb-aabf-862a518c1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubetriangle_qa_interim_dir = str(\n",
    "    app_config[\"interim_dir\"][\"cubetriangle_qa\"])\n",
    "#cubetriangle_instruction_response_interim_dir = str(\n",
    "    #here(app_config[\"interim_dir\"][\"cubetriangle_instruction_response\"]))\n",
    "tokenizer_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17778a37-ea43-4476-8f63-acf03e1b8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_the_data(examples,\n",
    "                      tokenizer,\n",
    "                      tokenizer_max_length: int = tokenizer_max_length,\n",
    "                      column_names: List = [\"question\", \"answer\"],\n",
    "                      data_type: str = \"cubetriangle\"\n",
    "                      ):\n",
    "    # if \"question\" in examples and \"answer\" in examples:\n",
    "    if data_type == \"cubetriangle\":\n",
    "        text = examples[column_names[0]][0] + examples[column_names[1]][0]\n",
    "    elif data_type == \"guanaco\":\n",
    "        text = examples[\"text\"][0]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid data_type. Supported values are 'cubetriangle' and 'guanaco'.\")\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        tokenizer_max_length\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1840275c-4af1-4688-ab09-42c5ae1ff547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cubetrianlge_qa_dataset(tokenizer,\n",
    "                                    tokenizer_max_length: int = tokenizer_max_length,\n",
    "                                    column_names: List = [\n",
    "                                        \"question\", \"answer\"],\n",
    "                                    data_dir: str = cubetriangle_qa_interim_dir,\n",
    "                                    data_type: str = \"cubetriangle\"\n",
    "                                    ):\n",
    "    finetuning_dataset = load_dataset(\n",
    "        'json', data_files=data_dir, split=\"train\")\n",
    "    print(\"Raw dataset shape:\", finetuning_dataset)\n",
    "    # Define a partial function with fixed arguments\n",
    "    partial_tokenize_function = partial(\n",
    "        tokenize_the_data,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_max_length=tokenizer_max_length,\n",
    "        column_names=column_names,\n",
    "        data_type=data_type\n",
    "    )\n",
    "    # print(\"Processed data description:\\n\")\n",
    "    # print(finetuning_dataset)\n",
    "    # print(\"---------------------------\")\n",
    "    tokenized_dataset = finetuning_dataset.map(\n",
    "        partial_tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "        drop_last_batch=True\n",
    "    )\n",
    "    tokenized_dataset = tokenized_dataset.add_column(\n",
    "        \"labels\", tokenized_dataset[\"input_ids\"])\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb4d40-53f5-4598-9676-225130904234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gemma Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
